{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment_analysis_2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch \n",
    "# Load dataset\n",
    "dataset = load_dataset('csv', data_files='labeled_reviews_en.csv')\n",
    "# Convert to pandas DataFrame\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Split dataset into train and test DataFrames\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "\n",
    "# Convert DataFrames back to Hugging Face datasets\n",
    "train_dataset = dataset['train'].class_encode_column(\"label\").from_pandas(train_df)\n",
    "test_dataset = dataset['train'].class_encode_column(\"label\").from_pandas(test_df)\n",
    "\n",
    "labels = train_dataset['label']  # This should be an array of integers representing your labels\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 351/351 [00:00<00:00, 25299.46 examples/s]\n",
      "Map: 100%|██████████| 40/40 [00:00<00:00, 10052.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Creating a DatasetDict\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 4\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    num_labels=num_labels,ignore_mismatched_sizes=True,\n",
    "    id2label={0: \"None\", 1: \"Other\", 2: \"Incorrect size\", 3: \"Lack of instructions\"},\n",
    "    label2id={\"None\": 0, \"Other\": 1, \"Incorrect size\": 2, \"Lack of instructions\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 17.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/review_classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Define metric for evaluation\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "## Initialize Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # Convert class weights to the same device as logits\n",
    "        class_weights_device = class_weights.to(logits.device)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_device)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 10%|█         | 44/440 [00:21<05:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0518457889556885, 'eval_accuracy': 0.725, 'eval_runtime': 0.3959, 'eval_samples_per_second': 101.037, 'eval_steps_per_second': 12.63, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 88/440 [00:35<01:30,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.995893657207489, 'eval_accuracy': 0.7, 'eval_runtime': 0.2863, 'eval_samples_per_second': 139.727, 'eval_steps_per_second': 17.466, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 30%|███       | 132/440 [00:50<01:18,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2797338962554932, 'eval_accuracy': 0.7, 'eval_runtime': 0.2873, 'eval_samples_per_second': 139.204, 'eval_steps_per_second': 17.4, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 176/440 [01:04<01:07,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1215875148773193, 'eval_accuracy': 0.775, 'eval_runtime': 0.2861, 'eval_samples_per_second': 139.809, 'eval_steps_per_second': 17.476, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 220/440 [01:18<00:59,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0500085353851318, 'eval_accuracy': 0.775, 'eval_runtime': 0.2879, 'eval_samples_per_second': 138.953, 'eval_steps_per_second': 17.369, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 264/440 [01:33<00:45,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9944307208061218, 'eval_accuracy': 0.775, 'eval_runtime': 0.2859, 'eval_samples_per_second': 139.908, 'eval_steps_per_second': 17.489, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|███████   | 308/440 [01:47<00:33,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1747077703475952, 'eval_accuracy': 0.75, 'eval_runtime': 0.3055, 'eval_samples_per_second': 130.943, 'eval_steps_per_second': 16.368, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 352/440 [02:01<00:22,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1956474781036377, 'eval_accuracy': 0.775, 'eval_runtime': 0.2856, 'eval_samples_per_second': 140.033, 'eval_steps_per_second': 17.504, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 90%|█████████ | 396/440 [02:15<00:11,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2687416076660156, 'eval_accuracy': 0.775, 'eval_runtime': 0.2868, 'eval_samples_per_second': 139.491, 'eval_steps_per_second': 17.436, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 440/440 [02:30<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2940075397491455, 'eval_accuracy': 0.775, 'eval_runtime': 0.2849, 'eval_samples_per_second': 140.386, 'eval_steps_per_second': 17.548, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [02:33<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 153.6133, 'train_samples_per_second': 22.85, 'train_steps_per_second': 2.864, 'train_loss': 0.4938087463378906, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=440, training_loss=0.4938087463378906, metrics={'train_runtime': 153.6133, 'train_samples_per_second': 22.85, 'train_steps_per_second': 2.864, 'total_flos': 126264740565600.0, 'train_loss': 0.4938087463378906, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models/review_classifier\")\n",
    "\n",
    "# To load:\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./models/review_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Lack of instructions', 'score': 0.5856142640113831}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# review = \"Nice cut but I only recommend it to experienced hares. It's a pity that there is no information about the addition of food. You only find out about this after you buy it, so anyone who buys it should bear this in mind. Therefore, one point deduction.\"\n",
    "review = \"Very easy to knit!\"\n",
    "review = \"Instructions were quite complicated, I couldn't do it.\"\n",
    "print(classifier(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from tqdm import trange\n",
    "data_reviews = pd.read_csv('reviews_with_sentiment3.csv')\n",
    "data_reviews[\"problem_category\"] = \"\"\n",
    "num_reviews = len(data_reviews[\"text\"])\n",
    "for i in trange(num_reviews): \n",
    "    designer_review = data_reviews[\"translated_text\"][i]\n",
    "    problem_category = classifier(designer_review)[0][\"label\"]\n",
    "    if problem_category == \"None\":\n",
    "        problem_category = \"No problem\"\n",
    "    data_reviews.loc[i, \"problem_category\"] = problem_category\n",
    "\n",
    "data_reviews.to_csv(\"reviews_with_sentiment_with_problem_category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reviews2 = pd.read_csv(\"reviews_with_sentiment_with_problem_category.csv\")\n",
    "data_reviews2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
